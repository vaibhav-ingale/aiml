FROM jupyter/all-spark-notebook:python-3.10.11
# FROM jupyter/pyspark-notebook:lab-4.0.7

# Build arguments for version management
ARG SPARK_VERSION="4.0.1"
ARG HADOOP_VERSION="3"
ARG AWS_JAVA_SDK_VERSION="1.12.565"
ARG HADOOP_AWS_VERSION="3.3.6"
ARG PYSPARK_VERSION="4.0.1"
ARG DELTA_SPARK_VERSION="4.0.0"
ARG PY4J_VERSION="0.10.9.9"
ARG POLARS_VERSION="1.33.1"

USER root

# Install curl and wget
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Download and install Spark
ENV SPARK_VERSION=${SPARK_VERSION}
ENV HADOOP_VERSION=${HADOOP_VERSION}
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# AWS S3A connector versions compatible with Spark
ENV AWS_JAVA_SDK_VERSION=${AWS_JAVA_SDK_VERSION}
ENV HADOOP_AWS_VERSION=${HADOOP_AWS_VERSION}

RUN apt-get update && apt-get install -y python3.10 python3.10-dev python3.10-distutils
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
RUN apt-get install -y python3-pip
# Upgrade pip to the latest version
RUN python3 -m pip install --upgrade pip
# spark-4.0.0-bin-hadoop3.tgz 
RUN wget -q "https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && tar xzf "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -C /opt/ \
    && mv "/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" "$SPARK_HOME" \
    && rm "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && chown -R jovyan:users $SPARK_HOME

USER $NB_UID

# Install Python packages for Spark and data analysis
RUN pip install --no-cache-dir \
    pyspark==${PYSPARK_VERSION} \
    delta-spark==${DELTA_SPARK_VERSION} \
    deltalake \
    polars==${POLARS_VERSION} \
    findspark \
    pandas \
    mlflow \
    numpy \
    matplotlib \
    seaborn \
    plotly \
    jupyter-dash \
    py4j==${PY4J_VERSION} \
    boto3 \
    minio \
    s3fs

# Configure PySpark environment
ENV PYTHONPATH=""
ENV PYTHONPATH="${SPARK_HOME}/python/:${PYTHONPATH}"
ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-${PY4J_VERSION}-src.zip:${PYTHONPATH}"
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS=lab

# Create directories
RUN mkdir -p /home/jovyan/work/notebooks
RUN mkdir -p /home/jovyan/data
RUN mkdir -p /home/jovyan/.ipython/profile_default/startup

# Copy IPython startup scripts
COPY --chown=jovyan:users startup/*.py /home/jovyan/.ipython/profile_default/startup/

# Set working directory
WORKDIR /home/jovyan/work

# Start Jupyter Lab
CMD ["start-notebook.sh", "--NotebookApp.token='spark123'", "--NotebookApp.password=''"]