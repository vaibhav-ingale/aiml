FROM jupyter/all-spark-notebook:python-3.10.11
# FROM jupyter/pyspark-notebook:lab-4.0.7


USER root

# Install curl and wget
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Download and install Spark 4.0.0
ENV SPARK_VERSION=4.0.1
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# AWS S3A connector versions compatible with Spark 4.0.1
ENV AWS_JAVA_SDK_VERSION=1.12.565
ENV HADOOP_AWS_VERSION=3.3.6

RUN apt-get update && apt-get install -y python3.10 python3.10-dev python3.10-distutils
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
RUN apt-get install -y python3-pip
# Upgrade pip to the latest version
RUN python3 -m pip install --upgrade pip
# spark-4.0.0-bin-hadoop3.tgz 
RUN wget -q "https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && tar xzf "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -C /opt/ \
    && mv "/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" "$SPARK_HOME" \
    && rm "spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
    && chown -R jovyan:users $SPARK_HOME

# Download AWS S3A connector JARs
RUN wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar" -O "${SPARK_HOME}/jars/hadoop-aws-${HADOOP_AWS_VERSION}.jar" \
    && wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_VERSION}.jar" -O "${SPARK_HOME}/jars/aws-java-sdk-bundle-${AWS_JAVA_SDK_VERSION}.jar"

USER $NB_UID

# Install Python packages for Spark 4.0 and data analysis
RUN pip install --no-cache-dir \
    pyspark==4.0.1 \
    findspark \
    pandas \
    numpy \
    matplotlib \
    seaborn \
    plotly \
    jupyter-dash \
    py4j==0.10.9.9 \
    boto3 \
    minio \
    s3fs

# Configure PySpark environment for Spark 4.0.0
ENV PYTHONPATH="${SPARK_HOME}/python/:$PYTHONPATH"
ENV PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.10.9.9-src.zip:$PYTHONPATH"
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=jupyter
ENV PYSPARK_DRIVER_PYTHON_OPTS=lab

# Create directories
RUN mkdir -p /home/jovyan/work/notebooks
RUN mkdir -p /home/jovyan/data

# Set working directory
WORKDIR /home/jovyan/work

# Start Jupyter Lab
CMD ["start-notebook.sh", "--NotebookApp.token='spark123'", "--NotebookApp.password=''"]